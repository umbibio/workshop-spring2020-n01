{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source\n",
    "\n",
    "http://inoryy.com/post/tensorflow2-deep-reinforcement-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CbnuaNU_X4RI"
   },
   "source": [
    "# Advantage Actor-Critic with TensorFlow 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "65TGe5QvX4RQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fHvM00xWX4RT",
    "outputId": "04ba1226-b9e5-4745-980d-1d4bf7b2c324",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import logging\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.layers as kl\n",
    "import tensorflow.keras.losses as kls\n",
    "import tensorflow.keras.optimizers as ko\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"TensorFlow Ver: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "S1LdC9PcX4Rg",
    "outputId": "86b009c7-47c4-4089-b62d-ab75c25fcdcd",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Eager by default!\n",
    "print(\"Eager Execution:\", tf.executing_eagerly())\n",
    "print(\"1 + 2 + 3 + 4 + 5 =\", tf.reduce_sum([1, 2, 3, 4, 5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z--NNwIlX4Rr"
   },
   "source": [
    "## Policy & Value Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UgwyNQCmX4Ru",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class ProbabilityDistribution(tf.keras.Model):\n",
    "  def call(self, logits, **kwargs):\n",
    "    # Sample a random categorical action from the given logits.\n",
    "    return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n",
    "\n",
    "\n",
    "class Model(tf.keras.Model):\n",
    "  def __init__(self, num_actions):\n",
    "    super().__init__('mlp_policy')\n",
    "    # Note: no tf.get_variable(), just simple Keras API!\n",
    "    self.hidden1 = kl.Dense(128, activation='relu')\n",
    "    self.hidden2 = kl.Dense(128, activation='relu')\n",
    "    self.value = kl.Dense(1, name='value')\n",
    "    # Logits are unnormalized log probabilities.\n",
    "    self.logits = kl.Dense(num_actions, name='policy_logits')\n",
    "    self.dist = ProbabilityDistribution()\n",
    "\n",
    "  def call(self, inputs, **kwargs):\n",
    "    # Inputs is a numpy array, convert to a tensor.\n",
    "    x = tf.convert_to_tensor(inputs)\n",
    "    # Separate hidden layers from the same input tensor.\n",
    "    hidden_logs = self.hidden1(x)\n",
    "    hidden_vals = self.hidden2(x)\n",
    "    return self.logits(hidden_logs), self.value(hidden_vals)\n",
    "\n",
    "  def action_value(self, obs):\n",
    "    # Executes `call()` under the hood.\n",
    "    logits, value = self.predict_on_batch(obs)\n",
    "    action = self.dist.predict_on_batch(logits)\n",
    "    # Another way to sample actions:\n",
    "    #   action = tf.random.categorical(logits, 1)\n",
    "    # Will become clearer later why we don't use it.\n",
    "    return np.squeeze(action, axis=-1), np.squeeze(value, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZQYPfmEfX4R3",
    "outputId": "f37a4dc8-cf02-4edb-8df2-0bb602399bfe",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Verify everything works by sampling a single action.\n",
    "env = gym.make('CartPole-v0')\n",
    "model = Model(num_actions=env.action_space.n)\n",
    "model.action_value(env.reset()[None, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9crzoKRjX4SB"
   },
   "source": [
    "## Advantage Actor-Critic Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "okotBpPdX4SE",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "class A2CAgent:\n",
    "  def __init__(self, model, lr=7e-3, gamma=0.99, value_c=0.5, entropy_c=1e-4):\n",
    "    # `gamma` is the discount factor; coefficients are used for the loss terms.\n",
    "    self.gamma = gamma\n",
    "    self.value_c = value_c\n",
    "    self.entropy_c = entropy_c\n",
    "\n",
    "    self.model = model\n",
    "    self.model.compile(\n",
    "      optimizer=ko.RMSprop(lr=lr),\n",
    "      # Define separate losses for policy logits and value estimate.\n",
    "      loss=[self._logits_loss, self._value_loss])\n",
    "\n",
    "  def train(self, env, batch_sz=64, updates=250):\n",
    "    # Storage helpers for a single batch of data.\n",
    "    actions = np.empty((batch_sz,), dtype=np.int32)\n",
    "    rewards, dones, values = np.empty((3, batch_sz))\n",
    "    observations = np.empty((batch_sz,) + env.observation_space.shape)\n",
    "    # Training loop: collect samples, send to optimizer, repeat updates times.\n",
    "    ep_rewards = [0.0]\n",
    "    next_obs = env.reset()\n",
    "    for update in range(updates):\n",
    "      for step in range(batch_sz):\n",
    "        observations[step] = next_obs.copy()\n",
    "        actions[step], values[step] = self.model.action_value(next_obs[None, :])\n",
    "        next_obs, rewards[step], dones[step], _ = env.step(actions[step])\n",
    "\n",
    "        ep_rewards[-1] += rewards[step]\n",
    "        if dones[step]:\n",
    "          ep_rewards.append(0.0)\n",
    "          next_obs = env.reset()\n",
    "          logging.info(\"Episode: %03d, Reward: %03d\" % (len(ep_rewards) - 1, ep_rewards[-2]))\n",
    "\n",
    "      _, next_value = self.model.action_value(next_obs[None, :])\n",
    "      returns, advs = self._returns_advantages(rewards, dones, values, next_value)\n",
    "      # A trick to input actions and advantages through same API.\n",
    "      acts_and_advs = np.concatenate([actions[:, None], advs[:, None]], axis=-1)\n",
    "      # Performs a full training step on the collected batch.\n",
    "      # Note: no need to mess around with gradients, Keras API handles it.\n",
    "      losses = self.model.train_on_batch(observations, [acts_and_advs, returns])\n",
    "      logging.debug(\"[%d/%d] Losses: %s\" % (update + 1, updates, losses))\n",
    "\n",
    "    return ep_rewards\n",
    "\n",
    "  def test(self, env, render=False):\n",
    "    obs, done, ep_reward = env.reset(), False, 0\n",
    "    while not done:\n",
    "      action, _ = self.model.action_value(obs[None, :])\n",
    "      obs, reward, done, _ = env.step(action)\n",
    "      ep_reward += reward\n",
    "      if render:\n",
    "        env.render()\n",
    "    return ep_reward\n",
    "\n",
    "  def _returns_advantages(self, rewards, dones, values, next_value):\n",
    "    # `next_value` is the bootstrap value estimate of the future state (critic).\n",
    "    returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n",
    "    # Returns are calculated as discounted sum of future rewards.\n",
    "    for t in reversed(range(rewards.shape[0])):\n",
    "      returns[t] = rewards[t] + self.gamma * returns[t + 1] * (1 - dones[t])\n",
    "    returns = returns[:-1]\n",
    "    # Advantages are equal to returns - baseline (value estimates in our case).\n",
    "    advantages = returns - values\n",
    "    return returns, advantages\n",
    "\n",
    "  def _value_loss(self, returns, value):\n",
    "    # Value loss is typically MSE between value estimates and returns.\n",
    "    return self.value_c * kls.mean_squared_error(returns, value)\n",
    "\n",
    "  def _logits_loss(self, actions_and_advantages, logits):\n",
    "    # A trick to input actions and advantages through the same API.\n",
    "    actions, advantages = tf.split(actions_and_advantages, 2, axis=-1)\n",
    "    # Sparse categorical CE loss obj that supports sample_weight arg on `call()`.\n",
    "    # `from_logits` argument ensures transformation into normalized probabilities.\n",
    "    weighted_sparse_ce = kls.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    # Policy loss is defined by policy gradients, weighted by advantages.\n",
    "    # Note: we only calculate the loss on the actions we've actually taken.\n",
    "    actions = tf.cast(actions, tf.int32)\n",
    "    policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n",
    "    # Entropy loss can be calculated as cross-entropy over itself.\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    entropy_loss = kls.categorical_crossentropy(probs, probs)\n",
    "    # We want to minimize policy and maximize entropy losses.\n",
    "    # Here signs are flipped because the optimizer minimizes.\n",
    "    return policy_loss - self.entropy_c * entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6x1u4jljX4SO",
    "outputId": "7c9d37fe-3484-4b0e-e3bd-761e28fd4eeb",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Verify everything works with random weights.\n",
    "agent = A2CAgent(model)\n",
    "rewards_sum = agent.test(env)\n",
    "print(\"Total Episode Reward: %d out of 200\" % agent.test(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M6VPZO0pX4SX"
   },
   "source": [
    "## Training A2C Agent & Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TQqGyP65X4Sa",
    "outputId": "d4f53314-4db8-413b-de5d-0eeed5edeb51",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# set to logging.WARNING to disable logs or logging.DEBUG to see losses as well\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "model = Model(num_actions=env.action_space.n)\n",
    "agent = A2CAgent(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TQqGyP65X4Sa",
    "outputId": "d4f53314-4db8-413b-de5d-0eeed5edeb51",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "rewards_history = agent.train(env)\n",
    "print(\"Finished training! Testing...\")\n",
    "print(\"Total Episode Reward: %d out of 200\" % agent.test(env))\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "plt.plot(np.arange(0, len(rewards_history), 5), rewards_history[::5])\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = VideoRecorder(env, base_path='./video1_trained')\n",
    "obs, done, ep_reward = env.reset(), False, 0\n",
    "while not done:\n",
    "    action, _ = agent.model.action_value(obs[None, :])\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    ep_reward += reward\n",
    "    rec.capture_frame()\n",
    "rec.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "actor-critic-agent-with-tensorflow2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
